%----------------------------------------------------------------------------------------
%	CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt,a4paper,oneside]{article}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{lipsum}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage[a4paper,left=2cm,right=2cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[noend]{algpseudocode}
\usepackage{algorithmicx}
\usepackage[Algoritmo]{algorithm}

%----------------------------------------------------------------------------------------
%	INFORMATION
%----------------------------------------------------------------------------------------

\title{Métodos de pesquisa no jogo de \textit{Sesqui}\\
  \vspace{0.1in}
  \large{Inteligência Artificial - Trabalho 2}
}

\author{Miguel Ferreira\footnote{Miguel Ferreira - 201304200}, Pedro Paredes\footnote{Pedro Paredes - 201205725}, DCC - FCUP}

\date{Abril 2015}

\renewcommand{\tablename}{Tabela}
\renewcommand{\figurename}{Figura}
\renewcommand{\refname}{Referências}

\begin{document}

\maketitle

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Introdução}
\label{sec:intro}

A existência de jogos surge naturalmente em muitas situações do
quotidiano. Em economia uma interação entre duas ou mais entidades
competidoras é normalmente vista como um jogo onde cada uma tenta
maximizar o seu lucro ou proveito. Porém, qualquer situação onde haja
cooperação ou conflito pode ser modelada como um jogo e as técnicas a
aplicar são semelhantes. Pode-se dizer assim que o estudo de jogos é o
estudo de processos de decisão.

Neste relatório exploraremos um subconjunto muito específico de jogos,
nomeadamente jogos determinísticos, discretos, de informação completa
e \textit{zero-sum} (embora alguns dos métodos se apliquem a muitos
outros tipos de jogos). A última característica é talvez a mais
importante e o que diz, informalmente, é que ``a sorte de uns é a
desgraça de outros'', o que significa que os recursos do jogo são
contantes, ou seja, o ganho de um jogador é exatamente a perda dos
seus oponentes. Mais especificamente explorar-se-á o jogo
\textit{Sesqui}, que é um jogo de tabuleiro de 2 jogadores e que será
descrito com maior pormenor numa secção futura.

O objetivo do relatório é de estudar métodos que permitam jogar
competitivamente jogos como os descritos a cima. Sendo assim
exploram-se 3 métodos diferentes: o algoritmo \textit{Minimax}, o
algoritmo de \textit{Alpha-Beta Pruning} e a \textit{Monte Carlo Tree
  Search}.

O resto do relatório está organizado da seguinte forma. Na Secção
\ref{sec:algconc} apresentamos uma descrição teórica dos 3 métodos de
otimizações comuns a cada um. Na Secção \ref{sec:sesqui} apresenta-se
o jogo \textit{Sesqui}, alguns resultados relacionados assim como uma
motivação para a implementação. Na Secção \ref{sec:imp} descreve-se a
nossa implementação dos métodos da Secção \ref{sec:algconc} e como
foram usados, assim como as heurísticas aplicadas ao
\textit{Sesqui}. Na Secção \ref{sec:resdes} discutem-se os resultados
e o desempenho da implementação. Finalmente, na Secção \ref{sec:conc}
é feita uma nota final sobre o trabalho e o relatório.

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Algoritmos e Conceitos de Pesquisa em Jogos \textit{zero-sum}}
\label{sec:algconc}

Existem várias maneiras de abordar um jogo. Para jogos mais simples
como o \textit{Nim} existe uma estratégia vencedora, para a maior
parte das posições iniciais, que se baseia num invariante do
jogo. Para jogos que sejam facilmente decompostos em subjogos
independentes, o teorema de \textit{Sprague-Grundy} pode ser usado
para obter uma estratégia. Para certos jogos, como o \textit{Connect
  Four} (conhecido em português pelo ``4 em linha'')
\cite{Allis:1988}, foram aplicados conhecimentos de combinatória para
resolver o jogo, ou seja, para obter uma estratégia vencedora do
jogo.

Porém, para a maior parte dos jogos, não é fácil obter resultados como
estes. É necessário assim recorrer a métodos exaustivos, que enumeram
os vários estados possíveis do jogo. Nesta secção descrevemos 3
métodos com este comportamento. A ideia geral deste métodos é de a
partir de uma configuração inicial enumerar as várias transições
possíveis com o objetivo de explorar o espaço de estados disponível
até chegar a posições terminais. Como este espaço de estados pode ser
demasiado extenso, recorrem-se a heurísticas para medir o valor de
configurações não terminais e assim efetua-se a pesquisa apenas até
uma certa profundidade.

Para simplificar a discussão nesta secção e nas próximas, introduzimos
alguns termos a usar. Um estado de um jogo é chamado de configuração
ou tabuleiro. O valor da função heurística para um determinado
tabuleiro é chamada da função utilidade. O \textit{payoff} é o valor
relativo de um tabuleiro (distinguimo-lo da função utilidade porque
consideramo-lo um valor teórico, enquanto que a função utilidade é um
valor empírico dado pela heurística). Na Subsecção \ref{ssec:mcts} é
descrito um algoritmo em árvore, por isso usa-se a notação padrão de
métodos de pesquisa (uma árvore cujos elementos são nós, o nó inicial
é a raiz e os nós mais profundos são as folhas \ldots).

\subsection{O Teorema e o Algoritmo Minimax}

%Cite something about Von Neumann
O teorema de \textit{Minimax} garante a existência de um valor $V$ e
uma estratégia para cada jogador de um jogo \emph{zero-sum}, de
maneira a que o primeiro jogador tenha um \emph{payoff} ótimo de $V$ e
o segundo um \emph{payoff} ótimo de $-V$, dadas as estratégias dos
seus oponentes.

O nome ``Minimax'' resulta da estratégia de minimizar o \emph{payoff}
máximo do jogador oponente e, dado a soma-zero do jogo, maximizar o
próprio \emph{payoff}.

O algoritmo de Minimax aplica a ideia descrita pelo teorema através de
uma busca em profundidade, devolvendo assim o melhor \emph{payoff}
para o um dado estado do jogo. Este algoritmo tem complexidade
exponencial de $O(b^{h})$, sendo $b$ o fator de ramificação do jogo e
$h$ um limite superior do número máximo de jogadas.

\begin{algorithm}
\begin{algorithmic}
\Function{Minimax}{$Node$, $Player$} \Comment{Devolve o valor minimax do melhor nó seguinte}
  \If{\Call{Terminal}{$Node$}} \Return{\Call{Value}{$Node$}}
  \EndIf
  \If{$Player=min$}
      \State $best \gets \infty$
      \For{$son$ in \Call{Successors}{$Node$}}
          \State $next \gets$ \Call{Minimax}{$son$, $max$}
          \State $best \gets$ \Call{Min}{$best$, $next$}
      \EndFor
  \ElsIf{$Player=max$}
      \State $best \gets -\infty$
      \For{$son$ in \Call{Successors}{$Node$}}
          \State $next \gets$ \Call{Minimax}{$son$, $min$}
          \State $best \gets$ \Call{Max}{$best$, $next$}
      \EndFor
  \EndIf
  \State\Return{$best$} 
\EndFunction
\end{algorithmic}
\caption{Minimax}
\end{algorithm}

\subsubsection{O \textit{Negamax}}

O algoritmo \textit{Negamax} é uma variante simplificada do \textit{Minimax} para jogos
que verificam uma simetria entre os valores do estado para cada um dos
jogadores. Sendo assim, é possível minimizar ou maximizar valores do
oponente através da negação do valor \textit{Negamax} da jogada seguinte.

\begin{algorithm}
\begin{algorithmic}
\Function{Negamax}{$Node$} \Comment{Devolve o valor do melhor nó seguinte}
  \If{\Call{Terminal}{$Node$}} \Return{\Call{Value}{$Node$}}
  \EndIf
  \State $best \gets -\infty$
  \For{$son$ in \Call{Successors}{$Node$}}
      \State $next \gets -$\Call{Negamax}{$son$}
      \State $best \gets$ \Call{Max}{$best$, $next$}
  \EndFor
  \State\Return{$best$} 
\EndFunction
\end{algorithmic}
\caption{Negamax}
\end{algorithm}

\subsection{O Método \textit{Alpha-Beta Pruning}}

O algoritmo \emph{Alpha-Beta Pruning} é uma versão otimizada do
\textit{Minimax} que devolve o valor correto sem percorrer
necessariamente a árvore completa. Esta otimização consiste em não
expandir nós que tenham um valor pior que outros já vistos
anteriormente e efetivamente reduzir a árvore de pesquisa.

O nome ``Alpha-Beta'' deve-se ao uso de duas variáveis que,
respetivamente, espeficicam o limite inferior e superior dos valores
de nós que devem ser expandidos\cite{Russell:AI:2003}. A variável
$\alpha$ guarda o valor do melhor para o jogador ``max'' e $\beta$
guarda o melhor valor para o jogador ``min''.

Uma limitação deste algoritmo é a dependência da ordem ``correta'' na
análise dos nós sucessores. Se a lista sucessores for gerada de forma
a que os nós que terão pior valor sejam expandidos primeiro, este
algoritmo terá de percorrer a árvore completa.

\begin{algorithm}
\begin{algorithmic}
\Function{Alpha-Beta}{$Node$, $Player$, $\alpha$, $\beta$}
  \If{\Call{Terminal}{$Node$}} \Return{\Call{Value}{$Node$}}
  \EndIf
  \If{$Player=min$}
      \State $best \gets \infty$
      \For{$son$ in \Call{Successors}{$Node$}}
          \State $next \gets$ \Call{Alpha-Beta}{$son$, $max$, $\alpha$, $\beta$}
          \State $best \gets$ \Call{Min}{$best$, $next$}
          \If{$best \leq \alpha$}
              \State\Return{$best$}
          \EndIf
          \State $\beta \gets$ \Call{Min}{$\beta$, $best$}
      \EndFor
  \ElsIf{$Player=max$}
      \State $best \gets -\infty$
      \For{$son$ in \Call{Successors}{$Node$}}
          \State $next \gets$ \Call{Alpha-Beta}{$son$, $min$, $\alpha$, $\beta$}
          \State $best \gets$ \Call{Max}{$best$, $next$}
          \If{$best \geq \beta$}
              \State\Return{$best$}
          \EndIf
          \State $\alpha \gets$ \Call{Max}{$\alpha$, $best$}
      \EndFor
  \EndIf
  \State\Return{$best$} 
\EndFunction
\end{algorithmic}
\caption{\emph{Alpha-Beta Pruning}}
\end{algorithm}

\subsubsection{Otimizações Comuns}

Existem várias otimizações aos algoritmos acima descritos. Um exemplo
seria tabelas de transposição\footnote{do inglês: \emph{Transposition
    Tables}.} que permitem memorizar as posições pelas quais o
algoritmo de busca adversarial já passou e os seus respetivos
valores. Dessa forma, é possível evitar recalcular valores de nós já
calculados noutros ramos.

Tabelas de transposição são especialmente benéficas em pesquisas
iterativas, porém constituem um custo de memória elevado.

É possível otimizar o \emph{pruning} do \emph{Alpha-Beta} através de
\emph{Aspiration Windows}. Este método consiste na previsão e
utilização de um limite inferior e superior mais curto. Na
eventualidade o resultado verdadeiro se encontrar fora desta janela é
necessário refazer a pesquisa com um intervalo maior.

Parar a uma dada prufundidade máxima nem sempre é a melhor estratégia
para um agente artificial, porque o nó terminal pode causar grandes
mudanças de jogo e o agente não pode expandir os nós seguintes para
poder decidir se é uma jogada boa ou não.  \emph{Quiescence} é uma
melhoria à qualidade do resultado da pesquisa \emph{Alpha-Beta},
avaliando apenas movimentos que não causem grandes mudanças no jogo e
expandindo os restantes.

%(http://chessprogramming.wikispaces.com/Alpha-Beta#Enhancements)
%(http://www.arimaa.com/arimaa/papers/ThomasJakl/bc-thesis.pdf Section 3.1)

%Tópicos: Transposition Table, Iterative Deepening, Aspiration Windows, Move ordering, Quiescence Search
%Referir: NegaScout

\newpage

\subsection{\textit{Monte Carlo Tree Search}}
\label{ssec:mcts}

O algoritmo de \textit{Monte Carlo Tree Search} (a partir de agora
referenciado como \texttt{MCTS}) surgiu graças ao jogo de
\textit{Go}. Depois de muitos desenvolvimentos no ramo de Inteligência
artificial e de vários jogos a terem programas competitivos, o
\textit{Go} continuou a ``fugir'' a estes desenvolvimentos e até tão
cedo como 1998 um jogador profissional era capaz de vencer o melhor
programa com um \textit{handicap} de até 30 peças (num tabuleiro de 19
por 19).  Em 2005, baseado num trabalho desenvolvido em
1993\cite{Brugmann:1993}, surgiu um novo algoritmo que obteve
resultados muito melhores do que até então, a \texttt{MCTS}. Desde
então que já sofreu muitos desenvolvimentos e várias otimizações e é o
algoritmo com melhor performance em \textit{Go} e além disso foi
aplicado a vários outros jogos (como o \textit{Hex}, o \textit{Arimaa}
e até \textit{Poker}).

A ideia base é a de usar simulações pseudoaleatórias (chamadas de
\textit{playouts}) de jogos a partir de cada posição e verificar
quantas levam a resultados vencedores. Porém, algumas jogadas
``merecem'' mais atenção do que outras e assim são exploradas mais
vezes de maneira a ter a informação mais detalhada possível. O
algoritmo pode ser descrito com os seguintes 4 passos, que são
executados iterativamente múltiplas vezes:

\begin{itemize}
  \item \textbf{Seleção}: A partir do nó raiz (que representa o
    tabuleiro inicial), selecionar filhos sucessivos até chegar a uma
    folha;
  \item \textbf{Expansão}: Se o nó folha selecionado não for final,
    expande-o, criando os seus filhos e construindo as suas
    propriedades;
  \item \textbf{Simulação}: Simula um jogo pseudoaleatório até atingir
    uma configuração final;
  \item \textbf{Propagação}: Propaga a informação da simulação pelo nó
    folha selecionado e todos os seus nós antecessores;
\end{itemize}

A Figura \ref{fig:mcts} ilustra este processo. Este método no limite
converge para o \textit{Minimax}, isto é, se o número de iterações das
4 fases for suficientemente alto. Na prática normalmente o algoritmo
sofre um \textit{plateau} dependendo das otimizações usadas.

Algumas vantagens deste método em relação aos anteriormente
apresentados. O primeiro é que é um algoritmo \textit{anytime}, ou
seja, que pode ser parado a qualquer altura. Para jogos onde o
controlo de tempo é importante, esta propriedade ajuda a gerir o
tempo. Depois, não é necessária nenhuma heurística para guiar o
algoritmo (embora seja útil usar-se como se verá na Subsecção
\ref{sec:mcopt}). Além disso, é gerada uma árvore assimétrica
naturalmente. Ao contrário de métodos como o \textit{Alpha-Beta} que
cortam jogadas que não parecem promissoras, a \texttt{MCTS} explora
mais as que parecem mais promissoras, dando origem assim a uma árvore
assimétrica.

Contudo, existem também algumas desvantagens. Primeiro é mais lenta
que a \textit{Alpha-Beta}. Para se obter um bom resultado é necessário
perder mais tempo, em média, do que na \textit{Alpha-Beta}. Outra
desvantagem é que a árvore de procura tem de ser mantida em memória, o
que introduz mais uma limitação possível ao algoritmo. Existem outras
desvantagens que foram mais notadas com a implementação do algoritmo e
que serão descritas na Secção \ref{sec:imp}.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=1\textwidth]{mcts}
  \caption{A \textit{Monte Carlo Tree Search}}
  \label{fig:mcts}
\end{figure}

\subsubsection{\textit{Upper Confidence Bound} e o \textit{Multi-Armed Bandit}}

O problema principal do algoritmo anterior está na fase de
Seleção. Como é que se escolhe o próximo nó a explorar? Por um lado
pode ser boa ideia tentar explorar o que nós parece mais promissor de
momento, mas com isto pode-se deixar de lado nós que são muito mais
promissores (ficando preso num máximo local). Por outro lado pode-se
dar prioridade aos nós menos explorados, mas assim não estamos a tirar
partido da informação que vamos tendo (a árvore fica simétrica). Isto
é análogo ao problema clássico do \textit{Multi-Armed Bandit}, onde
dado um conjunto de \textit{slot-machines} com diferentes níveis de
recompensa, como é que se pode obter uma estratégia que maximize os
ganhos do jogo. É o dilema da exploração contra aproveitamento (em
inglês \textit{exploitation vs exploration}).

Existem várias maneiras de abordar este problema. Uma das primeiras a
ser aplicada a \texttt{MCTS} chama-se \textit{Upper Confidence
  Bound}. Este método seleciona cada nó uma vez
inicialmente. Posteriormente usa a seguinte fórmula de exploração e
escolhe o nó que a maximiza: $\bar{X_i} + c
\sqrt{\frac{\log{n}}{n_i}}$ para cada nó $i$, onde $\bar{X_i}$ é o
valor médio desse nó (que no contexto da \texttt{MCTS} é dado pelo
número de vezes que uma simulação a partir desse nó resultou numa
vitória, sobre o número total de vezes que o nó foi escolhido para uma
simulação), $n$ é o número total de vezes que foi efetuada uma seleção
que envolvesse aquele nó (que corresponde ao número de vezes que o pai
desse nó foi selecionado para uma simulação), $n_i$ corresponde ao
número de seleções para simulação desse nó e $c$ é a chamada constante
de \texttt{UCB} e que normalmente é $\sqrt{2}$. À combinação deste
formula com \texttt{MCTS} chama-se de \textit{Upper Confidence bound
  to Trees}. O uso deste método revelou-se muito positivo e desde
então já foi muito estudado e melhorado.

\subsubsection{Heurísticas: \textit{Heavy} e \textit{Light} \textit{Playouts}}

Os métodos descritos até à subsecção anterior constituem a base do que
é a \texttt{MCTS}. Este algoritmo elegante até agora não necessitou de
nenhum conhecimento específico do jogo para o guiar e por isso é um
\textit{General Playing Algorithm}. Porém, para jogos de média
complexidade e alta complexidade, estabiliza em níveis baixos de
desempenho e por isso não é muito competitivo. Por isso, é necessário
aplicar um conjunto de otimizações e técnicas para melhorar a sua
performance. Uma delas é introduzir heurísticas no algoritmo. É
possível introduzir heurísticas em várias das fases do algoritmo, aqui
descrevemos o seu uso na fase de Simulação.

É possível fazer os \textit{playouts} puramente aleatórios e escolher
cada jogada a seguir aleatoriamente. A isto chama-se \textit{light
  playouts}. De um modo geral não funcionam mal, porque
experimentalmente verificou-se que \textit{playouts} de valor inferior
(jogados de maneira mais \textit{naive}) levam a melhores resultados
do método geral (em comparação com métodos não perfeitos, claro que se
o \textit{playout} fosse perfeito o seu resultado dava a informação
completa e não eram necessários mais). Infelizmente, no caso geral são
normalmente necessárias demasiadas iterações para se obter um bom
resultado. Assim, foi introduzido conhecimento especifico do jogo para
guiar os \textit{playouts}. A isto chama-se \textit{heavy playouts}. O
método mais comum e que leva à maior eficiência é o chamado
\textit{Best-of-N}, que corresponde a gerar um conjunto de $N$ jogadas
possíveis em cada turno do \textit{playout} e escolher das $N$ a que
leva a um tabuleiro com valor de heurística superior. A única
desvantagem deste método é que torna a simulação mais lenta e por isso
obriga a diminuir o número de iterações, mas no geral os resultados
melhoram.

\subsubsection{Otimizações Comuns}
\label{sec:mcopt}

%(http://www.arimaa.com/arimaa/papers/ThomasJakl/bc-thesis.pdf Section 3.2)
%(http://arimaa.com/arimaa/papers/TomasKozelekThesis/mt.pdf Section 3.3)

Nesta subsecção introduzimos algumas das otimizações mais comuns e que
levam a melhores resultados com a \texttt{MCTS}. Como o espaço é
limitado (e o tempo também), descrevem-se apenas brevemente cada uma.

\begin{itemize}
\item \textbf{\textit{Progressive Bias}}: Esta otimização corresponde
  combinar informações estimadas com informações empíricas, ao
  introduzir um termo na fórmula de exploração \texttt{UCB} de
  $\frac{H_i}{n_i + 1}$, onde o $H_i$ corresponde a um coeficiente
  obtido a partir da heurística. Assim, a heurística ajuda a guiar
  quais os nós parecem promissores no início, quando ainda foram
  executados poucas iterações;
\item \textbf{\textit{Virtual Visits}}: Corresponde a iniciar o nó com
  um conjunto de visitas (o $n_i$ da fórmula de exploração) positivo,
  assim como número de \textit{playouts} com sucesso (que afeta a
  variável $\bar{X_i}$). Este número de visitas é uma constante $v$ e
  o número de \textit{playouts} com sucesso é dependente da heurística
  e será algo como $H_i * v$ (assumindo a heurística
  normalizada). Assim como a otimização anterior, ajuda a guiar a
  seleção no inicio e introduz algum valor heurístico nos nós;
\item \textbf{\textit{Maturity Threshold}}: Esta pequena otimização
  pode revelar-se muito importante em vários casos. A ideia é limitar
  a expansão de nós, só expandindo nós que já tenham tido pelo menos
  um número de visitas (ou um número de escolhas para simulação) maior
  ou igual a um coeficiente chamado coeficiente de
  maturidade. Normalmente também se usa a profundidade do nó para
  limitar a expansão, que combinado com o coeficiente, ajuda a não
  expandir nós demasiadamente profundos para terem impacto no valor
  dos nós mais superiores;
\item \textbf{\textit{Children Caching}}: Esta otimização pretende
  diminuir o tempo gasto no passo de seleção. A ideia é manter uma
  \textit{cache} com número pequeno de filhos do nó e só selecionar
  nós desta \textit{cache}. A \textit{cache} é preenchida com um
  conjunto dos melhores nós (com valor mais alto da função de
  exploração \texttt{UCB}) e é esvaziada e recalculada de tempo a
  tempo;
\item \textbf{\textit{Grandfather Heuristics}}: Neste otimização
  inicializa-se o número de visitas e o valor do nó com o número de
  visitas e o valor do nó do nó dois níveis acima (o antecessor do
  antecessor), caso exista. A ideia é que se uma jogada foi positiva,
  então a sua continuação também a será;
\item \textbf{\textit{Transposition Tables}}: Esta adaptação da
  otimização já existente na pesquisa \textit{Alpha-Beta} tem como
  objetivo evitar nós com tabuleiros repetidos, fazendo um
  \textit{merge} de nós com o mesmo tabuleiro;
\end{itemize}

Existem muitas mais otimizações e técnicas para melhorar a
\texttt{MCTS}. A mais importante não referida é a chamada
\textit{Rapid Action Value Estimation} que é muito usada em jogos de
colocação de peças (como o \textit{Go}) cujo objetivo é de considerar
a informação que uma jogada teve num \textit{playout} para todos os
tabuleiros que contenham essa jogada.

%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------


\section{O Jogo de \textit{Sesqui}}
\label{sec:sesqui}

Apresentamos agora o jogo sobre o qual incidiu a implementação. Para
permitir explorar o jogo melhor e testar a implementação, foi
implementado uma versão em \textit{web} do jogo que está disponível em
\url{http://sesqui-test.meteor.com}. O registo é simples e permite
jogar com um adversário e também jogar contra cada um dos
programas. Mais informações sobre os programas serão dadas na Secção
\ref{sec:imp}.

O \textit{Sesqui} tem poucas referências, mas aparentemente foi criado
pela Associação de Jogos Matemáticos Ludus
\cite{Ludus:2014}. Escolhemos este jogo porque é usado no Campeonato
Nacional de Jogos Matemáticos \cite{Ludus:2014b} a partir da edição
2015. Além disso é um jogo que nunca foi estudado e por isso tem muito
material para ser explorado.

Apesar disto, após vários jogos e testes, chegámos à conclusão que o
jogo tem algumas falhas de construção que são impeditivas para que
seja um jogo interessante como iremos ver. Mesmo assim foi um jogo
interessante para explorar.

\subsection{Regras do Jogo}

O jogo tem muitos traços do \textit{Hex}, mas joga-se num tabuleiro
quadriculado de 8 por 8, jogando um tabuleiro com peças brancas e
outro com peças pretas. Um dos jogadores pretende ligar o topo do
tabuleiro à base com uma cadeia de peças adjacentes da sua cor
(verticalmente). O outro jogador pretender ligar o lado esquerdo ao
lado direito com peças adjacentes da sua cor (horizontalmente). Duas
peças são consideradas adjacentes se se encontrarem numa casa a uma
distância de uma unidade, verticalmente, horizontalmente ou
diagonalmente.

O jogo começa com o jogador das peças brancas a colocar uma peça onde
quiser. Segue o jogador de peças pretas a colocar duas peças onde
quiser. A partir deste turno cada jogador tem de mover uma peça da sua
cor e colocar uma peça da sua cor, na ordem que quiser. A colocação da
peça tem de ser numa casa vazia adjacente na horizontal ou vertical de
uma outra peça sua. O movimento de uma peça é feita como o movimento
de uma dama no xadrez, não sendo possível saltar por cima de outras
peças ou de colocar numa posição com outra peça. Um exemplo de um jogo
encontra-se na Figura \ref{fig:sesqui}.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.3\textwidth]{sesqui_game}
  \caption{Um jogo de \textit{Sesqui}}
  \label{fig:sesqui}
\end{figure}

Para impedir empates, existe ainda uma regra que impede a existência
de peças cruzadas num padrão de 4 peças como o ilustrado na Figura
\ref{fig:cross}.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.3\textwidth]{sesqui_forb}
  \caption{Um padrão proibido de \textit{Sesqui}}
  \label{fig:cross}
\end{figure}

\subsection{Alguns Resultados sobre o Jogo}

%(Numero de tabuleiros, empates, vantagem do segundo)

Primeiro é de notar que não é possível haver empates. Existem algumas
posições atingíveis onde o jogador de peças brancas não tem onde jogar
(como jogar um quadrado de 4 por 4 num dos cantos do tabuleiro que o
jogador de peças pretas envolve com peças suas), mas fora isso não é
possível empatar. Não provamos isto, mas imaginamos que a prova é
semelhante à prova análoga para o \textit{Hex}. Experimentalmente, a
nossa implementação também não encontrou nenhum tabuleiro possível de
empate.

Um resultado desencorajante, é que aparentemente o segundo jogador tem
vantagem sobre o primeiro graças à colocação de duas peças no
início. Após vários jogos entre diferentes pessoas, existe uma clara
tendência para o jogador de peças pretas ganhar. O mesmo foi
confirmado pela implementação. Porém, não conseguimos provar este
resultado usando nenhuma das estratégias mais conhecidas para este
tipo de prova (como estratégia de roubo de estratégia).

\subsection{Intuição para a Implementação}

A nossa intuição para a implementação do jogo será que é um jogo muito
intermédio em termos de qual das técnicas funcionará melhor. No caso
do \textit{Alpha-Beta} será complicado que tenha um bom desempenho,
mesmo com uma boa função heurística, pois o fator de ramificação é
muito alto (em média pode chegar aos 1000, graças a haver um movimento
e uma colocação de peça por jogada). Já no caso da \texttt{MCTS}, é um
jogo que tem algumas características do \textit{Hex} e do \textit{Go},
que funcionam bem com \texttt{MCTS}, porém os movimentos de peças como
damas de xadrez formam um padrão que complica algumas das otimizações
da \texttt{MCTS}.

%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------

\section{Implementação}
\label{sec:imp}

Para testar as técnicas descritas nas secções anteriores, os 3 métodos
foram implementados em \texttt{C++}. Foi escolhida esta linguagem
principalmente por ser rápida, que é um fator muito limitante para
estes métodos. O código encontra-se anexo a este relatório.

O tabuleiro de jogo foi implementado usando \textit{bitmasking} para
aumentar a eficiência tanto a nível temporal como a nível espacial, na
manipulação do mesmo nos vários métodos.

Visto que no \textit{Sesqui} há duas jogadas por turno, a
implementação poderia usar uma geração de jogadas \textit{step-based},
ou seja, gerava-se cada uma dos movimentos individuais de um turno (e
no método retornam-se duas jogadas e não uma), ou \textit{turn-based},
onde uma jogada é um conjunto de movimentos (equivalente a um
turno). A primeira tem a vantagem de ser mais leve, mas a segunda
consegue potencialmente evitar jogadas redundantes (que levam ao mesmo
tabuleiro). No caso da \texttt{MCTS} optou-se pelo primeiro pela sua
simplicidade, nos outros dois métodos optou-se pelo segundo.

Para os 3 métodos o código implementado funciona recebendo um
tabuleiro em notação \texttt{PM}, que representa o conjunto de jogadas
feitas no seguinte formato: \texttt{p|x|y} para representar uma
colocação em $(x, y)$, \texttt{m|x0|y0|x1|y1} para representar um
movimento de uma peça de $(x_0, y_0)$ para $(x_1, y_1)$, as diferente
jogadas são separadas por \texttt{\$} (no terminal é preciso fazer o
\textit{escaping} deste carácter escrevendo
\texttt{\textbackslash\$}). O código devolve um par de jogadas em
notação \texttt{PM} também na versão não verbosa. Por ser desta forma,
não são aproveitados cálculos de uma jogada para a outra, o que no
caso da \texttt{MCTS} poderia ser muito proveitoso.

É de notar que a implementação do \textit{Minimax} é apenas de
demonstração e não foi usada para testes de desempenho por ser
demasiado simples (e não obtinha resultados relevantes em tempo
decente). Assim a discussão deste e das próximas secções incidirá mais
nos outros dois métodos.

\subsection{\textit{Minimax}}

A implementação do algoritmo \textit{Minimax} é baseada no pseudo-código de
Russell\cite{Russell:AI:2003}. Devido ao elevado fator de ramificação
do jogo, fomos obrigados a impor limites à pesquisa adversarial
\textit{Minimax}. Atualmente esse limite de profundidade é de 3 \emph{ply}.

Na mesma implementação de Minimax, adicionamos umas cláusulas para
permitir \emph{pruning} e efetivamente transformar o algoritmo em
\emph{Alpha-Beta Pruning}.

Ao atingir a profundidade máxima permitida, o valor do nó atual é
estimado por uma função heurística.

\subsection{Pesquisa \textit{Alpha-Beta}}

\lipsum[1]

\lipsum[2]

\subsection{\texttt{MCTS}}

A \texttt{MCTS} foi implementada baseada no código \cite{MCTSHub} com
6000 iterações por jogada. Foram implementadas várias otimizações das
descritas na Subsecção \ref{ssec:mcts}. Foi usada a
\textit{Progressive Bias} com a heurística a descrever na subsecção
seguinte. \textit{Virtual Visits} com uma constante de visitas $v =
6$. Foi aplicado um \textit{Maturity Threshold} de constante igual a
180. Foi também implementada uma \textit{Grandfather
  Heuristic}. Finalmente experimentamos diferentes configurações para
uma \textit{Children Caching}, mas não foi muito eficiente.

O foco desta implementação foi o de fazer os \textit{playouts} o mais
rápidos possível, pois estes eram o grande \textit{bottleneck} da
eficiência que impedia aumentar as iterações. Para melhorar a
eficiência, o \textit{playouts} é interrompido após 30 iterações e
usa-se ou o valor da heurística para o tabuleiro final, ou o valor do
vencedor. Estas iterações são fixas, no sentido em que mesmo que o
jogo acabe, continua a simulação. Isto é feito porque em média não
altera o resultado final, mas aumenta muito a eficiência.

Todas estas constantes e otimizações foram ajustadas empiricamente,
através de testes em posições específicas limite e de jogos manuais
contra o programa.

\subsection{Heurísticas}

\lipsum[1]

\lipsum[2]

\lipsum[3]

%----------------------------------------------------------------------------------------
%	SECTION 5
%----------------------------------------------------------------------------------------

\section{Resultados e Desempenho}
\label{sec:resdes}

Não foi possível testar os programas com grande rigor, visto que não
existe nenhuma comunidade de \textit{Sesqui} e por isso não existem
\textit{benchmarks} a seguir. Assim, o teste de desempenho é feito
avaliando o resultado de jogos contra os autores.

De um modo geral os programas atingiram um nível semelhante ao de um
jogador iniciante. No caso da \texttt{MCTS} certas configurações dos
parâmetros levaram a um desempenho superior que tinha resultados
surpreendentes por vezes, chegando até a ganhar um jogo contra um dos
autores a jogar em ``concentração máxima'' (a Figura \ref{fig:mctswin}
mostra o resultado final do jogo na plataforma \textit{web}).

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.8\textwidth]{sesqui_win}
  \caption{Um jogo interessante contra o \texttt{MCTS} (o programa jogou com as peças pretas)}
  \label{fig:mctswin}
\end{figure}

A implementação da pesquisa \textit{Alpha-Beta} não conseguiu ter nenhum
desempenho melhor, mas visto que a implementação continha poucas
otimizações e a função heurística não era muito complexa, não era
esperado um resultado muito melhor. Já a \texttt{MCTS} mostrou-se
forte em jogadas iniciais (mais estratégicas e de planeamento a longo
prazo) mas fraco em jogadas finais (mais táticas e de planeamento a
curto prazo). Isto poderá resultar que para jogadas perto da vitória é
fácil os \textit{playouts} favorecerem jogadas incorretas. Isto foi de
tal forma crítico que o programa chegava a falhar jogadas triviais
vencedoras.

Em termos de desempenho temporal, não foram exploradas muitas
opções. A versatilidade da \texttt{MCTS} permitia que num controlo de
tempo mais fixo, conseguisse gerir melhor o seu tempo e obter mais
resultados. Mesmo no caso da pesquisa \textit{Alpha-Beta}, algum tempo
extra significaria que era possível ir a uma profundidade maior e
aumentar imenso a sua performance. Ambos os programas foram escalados
para demorar entre 5 a 10 segundos para jogar, o que não é muito para
um programa competitivo em vários outros jogos.

%----------------------------------------------------------------------------------------
%	SECTION 6
%----------------------------------------------------------------------------------------

\section{Conclusão e Trabalho Futuro}
\label{sec:conc}

%Paralelismo, Otimizações no AB, MC-AB (juntar os dois)\cite{Dutra:2015}

Concluímos dizendo que obviamente ainda há muito a melhorar. Vários
afinamentos serão necessários em todos os métodos e novas otimizações
poderão melhorar muito o performance dos programas. Outra adição que
poderá melhorar muito os programas é a paralelização do código. Para
ambos os métodos já existe uma vasta literatura sobre
paralelismo. Outro objetivo futuro é o de juntar a \texttt{MCTS} com a
pesquisa \textit{Alpha-Beta} para tirar o melhor dos dois, em algo
parecido com uma \textit{Monte Carlo Alpha-Beta}.

Fazendo apenas uma nota final, depois da intuição estabelecida,
concluímos que a \texttt{MCTS} se comportou ligeiramente melhor,
embora tenha falhas, o que já era de esperar.

\bibliographystyle{plain}
\bibliography{lab_report_2}

\end{document}
